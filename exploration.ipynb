{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Security Prediction\n",
    "\n",
    "We look at SPY and forecast its value. We then compare the predictions by using them with simple trading strategies.\n",
    "\n",
    "[Jump to the conclusion at the end](#Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as learn # bug workaround: import first\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import clock\n",
    "\n",
    "# Disable any Xwindows backend\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Interactive plots\n",
    "# import mpld3\n",
    "# mpld3.enable_notebook()\n",
    "\n",
    "# Seaborn\n",
    "import seaborn as sb\n",
    "\n",
    "# Temporarily suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Profiling notebook perfomance\n",
    "start_notebook = clock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "import quandl\n",
    "symbol = 'SPY'\n",
    "\n",
    "df = quandl.get('GOOG/NYSE_' + symbol)['Close'].to_frame(name = symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a quick look\n",
    "df[symbol].plot()\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.gcf().savefig('figures/preprocessed.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract hour, month and year\n",
    "df['hour'] = df.index.hour\n",
    "df['month'] = df.index.month\n",
    "df['year'] = df.index.year\n",
    "# Which day of the week is this?\n",
    "df['dow'] = df.index.dayofweek\n",
    "# Is this a weekend?\n",
    "df['weekend'] = df.dow >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distribution of dates?\n",
    "dates = pd.Series(df.index)\n",
    "\n",
    "dates_min = pd.to_datetime(dates.min())\n",
    "dates_max = pd.to_datetime(dates.max())\n",
    "n_years = (dates_max - dates_min).days/365\n",
    "# n_years = (dates_max - dates_min).to_timedelta64()\n",
    "# n_years = n_years.astype('timedelta64[Y]')\n",
    "print(\"Dates range from {} to {} covering {:.1f} years\".format(dates.min(), dates.max(), n_years))\n",
    "\n",
    "dates.groupby(dates.dt.dayofweek).count().plot(kind = \"bar\", title = \"Count per day of week\")\n",
    "plt.show()\n",
    "# There are no changes during the weekends\n",
    "\n",
    "dates.gr-oupby(dates.dt.month).count().plot(kind = \"bar\", title = \"Count per mount\")\n",
    "plt.show()\n",
    "\n",
    "dates.groupby(dates.dt.year).count().plot(kind = \"bar\", title = \"Count per year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_describe = df.describe()\n",
    "df_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seasonal pattern?\n",
    "\n",
    "def seasonal_plot(series):\n",
    "    \"\"\"\n",
    "    Plot the seasonal trend.\n",
    "    \"\"\"\n",
    "    \n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "    # Yearly variation\n",
    "    decomposition = seasonal_decompose(series, model = 'additive', freq = 365)\n",
    "\n",
    "    decomposition.plot()\n",
    "    plt.show()\n",
    "\n",
    "seasonal_plot(df[symbol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rolling_stddev(series, window):\n",
    "    \"\"\"\n",
    "    Return rolling standard deviation of series.\n",
    "    \"\"\"\n",
    "    \n",
    "    dates = series.index\n",
    "    series_roll = series.rolling(window = window, min_periods = 1).std()\n",
    "\n",
    "    return series_roll.loc[dates]\n",
    "\n",
    "# Rolling std dev of difference\n",
    "# df['rolling_stddev'] = rolling_stddev(df['meanless'])\n",
    "# df['rolling_stddev'] = rolling_stddev(df['security'], window = 365*24*60)\n",
    "# df['rolling_stddev'].plot()\n",
    "\n",
    "security_stddev = rolling_stddev(df[symbol], 365)\n",
    "\n",
    "# security_stddev = rolling_stddev(meanless_subsampled, 365)\n",
    "security_stddev.plot(title = \"Standard deviation of security\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rolling means are important, so we can add different mean window to the data frame. We could use them as features for prediction, or as part of an [ARIMA model](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rolling_mean(series, window):\n",
    "    \"\"\"\n",
    "    Return rolling mean of series. \n",
    "    NOTE Shift by 1 to avoid including current value in prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    dates = series.index\n",
    "    series_roll = series.shift(1).rolling(window = window, min_periods = 1).mean()\n",
    "\n",
    "    return series_roll.loc[dates]\n",
    "\n",
    "# We attach a few rolling means to the data frame\n",
    "# We shift by one to avoid using the current value (for predic)\n",
    "df['mean_weekly'] = rolling_mean(df[symbol], 7)\n",
    "df['mean_monthly'] = rolling_mean(df[symbol], 30)\n",
    "df['mean_yearly'] = rolling_mean(df[symbol], 365)\n",
    "\n",
    "df[['mean_weekly', 'mean_monthly', 'mean_yearly']].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the mean and see what the rest looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rolling mean\n",
    "\n",
    "security_mean = rolling_mean(df[symbol], 365)\n",
    "\n",
    "# security_mean.plot()\n",
    "# plt.show()\n",
    "\n",
    "meanless_subsampled = df[symbol] - security_mean\n",
    "meanless_subsampled.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's also look at the frequency spectrum of this in case something comes up.\n",
    "\n",
    "def fourier_plot(series, amplitude = None):\n",
    "    \"\"\"\n",
    "    Plot the frequency spectrum of the time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(series)\n",
    "    sp = np.fft.fft(series)\n",
    "    freq = np.fft.fftfreq(n)\n",
    "\n",
    "    plt.plot(freq, sp.real, label = 'Re')\n",
    "    plt.plot(freq, sp.imag, label = 'Im')\n",
    "    \n",
    "    plt.legend()\n",
    "    if amplitude is not None:\n",
    "        plt.ylim(-amplitude, amplitude)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "fourier_plot(meanless_subsampled, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative\n",
    "Let's see how the derivative changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['derivative'] = df[symbol].diff()\n",
    "df['derivative'].plot(title = \"Derivative\")\n",
    "\n",
    "plt.yscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seasonal pattern?\n",
    "deriv = df['derivative'].fillna(method = 'pad').fillna(method = 'bfill')\n",
    "seasonal_plot(deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at rolling means to see if the trend observed above can be discerned with different windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We attach a few rolling means to the data frame\n",
    "df['mean_deriv_weekly'] = rolling_mean(df['derivative'], 7)\n",
    "df['mean_deriv_monthly'] = rolling_mean(df['derivative'], 30)\n",
    "df['mean_deriv_yearly'] = rolling_mean(df['derivative'], 365)\n",
    "\n",
    "df[['mean_deriv_weekly', 'mean_deriv_monthly', 'mean_deriv_yearly']].plot()\n",
    "# plt.yscale('symlog')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could consider predicting the derivative directly instead of the value of the security (and use the predicted derivative as the basis of a trading strategy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy Backtesting\n",
    "\n",
    "We use a simple framework from [here](https://www.quantstart.com/articles/Research-Backtesting-Environments-in-Python-with-pandas) to test simple strategies based on the forecast we develop. The framework assumes we have a time series of signal to buy, sell, or keep. For now, we do the simple strategy of buying/selling a few shares at a time, whenever the signal is &plusmn;1. The decisions must be based on the history of the security before the current time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backtest_portfolio(bars, signals, initial_capital = 100000.0, get_portfolio = False):\n",
    "    \"\"\"\n",
    "    Create a system that purchases n \n",
    "    of a particular symbol upon a long/short signal, \n",
    "    assuming the market price of a bar.\n",
    "    \n",
    "    The portfolio is constructed from the positions DataFrame \n",
    "    by assuming the ability to trade at the precise market open price\n",
    "    of each bar (unrealistic!). \n",
    "\n",
    "    In addition, there are zero transaction costs \n",
    "    and cash can be immediately borrowed for shorting \n",
    "    (no margin posting or interest requirements). \n",
    "    \"\"\"\n",
    "    \n",
    "    index = bars.index\n",
    "\n",
    "    # Create a 'positions' Series that simply longs or shorts n of the particular symbol \n",
    "    # based on the forecast signals of {1, 0, -1} from the signals DataFrame.\n",
    "    n = 1\n",
    "    signals = signals.fillna(0.)\n",
    "    positions = n * signals\n",
    "    \n",
    "    bars = bars.values.ravel()\n",
    "    positions = positions.values.ravel()\n",
    "    \n",
    "    # Calculate the total of cash and the holdings (market price of each position per bar), \n",
    "    # in order to generate an equity curve ('total') and a set of bar-based returns ('returns').\n",
    "    holdings = positions * bars # Price of current operation\n",
    "    cash = initial_capital - holdings.cumsum() # Cash available\n",
    "    total = cash.ravel() + positions.ravel().cumsum() * bars.ravel() # Total value\n",
    "            \n",
    "    # Return the return on initial investement from between the beginning and the end\n",
    "    V0 = initial_capital\n",
    "    Vf = total[-1]\n",
    "    pct = (Vf - V0)/V0\n",
    "    \n",
    "    if not get_portfolio:\n",
    "        # Return the return on initial investment\n",
    "        return pct\n",
    "    else:            \n",
    "        # Return the portfolio object to be used elsewhere.\n",
    "        pf = pd.DataFrame(index = index)\n",
    "        pf['positions'] = positions\n",
    "        pf['signals'] = signals\n",
    "        pf['holdings'] = holdings\n",
    "        pf['cash'] = cash\n",
    "        pf['total'] = total\n",
    "        pf['returns'] = pf['total'].pct_change() # Percent change for each step\n",
    "        \n",
    "        return pct, pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a few simple strategies based on difference between shifted or predicted values. For now, we buy/sell a fix number of shares, but we could buy/sell signal proportinally to the (scaled) difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy_with_forecast(outcome, predict):\n",
    "    \"\"\"\n",
    "    If current prediction higher than yesterday's value, buy. If lower, sell.\n",
    "    \"\"\"\n",
    "    \n",
    "    index = outcome.index\n",
    "\n",
    "    # NOTE memory error when not using values and ravel\n",
    "    predict = np.array(predict).ravel()\n",
    "    outcome_shifted = outcome.shift(1).values.ravel()\n",
    "    signal = np.sign(predict - outcome_shifted)\n",
    "    \n",
    "    signal = pd.Series(signal.ravel(), index = index)\n",
    "\n",
    "    # If current prediction higher than yesterday's prediction, buy. If lower, sell.\n",
    "    # predict = pd.Series(predict, index = outcome.index)\n",
    "    # signal = np.sign(predict - predict.shift(1))\n",
    "    \n",
    "    # Act proportionally to the difference\n",
    "    # FIXME fit/transform on train/test\n",
    "    \n",
    "    # from sklearn.preprocessing import StandardScaler\n",
    "    # scaler = StandardScaler()\n",
    "    # signal = scaler.fit_transform(predict - outcome.shift(1))\n",
    "    # signal = round(signal)\n",
    "    \n",
    "    return signal\n",
    "\n",
    "def strategy_with_recent_history(outcome):\n",
    "    \"\"\"\n",
    "    If going up in the last two days, buy. If going down, sell.\n",
    "    \"\"\"\n",
    "    \n",
    "    signal = np.sign(outcome.shift(1) - outcome.shift(2))\n",
    "    \n",
    "    # Act proportionally to the difference\n",
    "    # FIXME fit/transform on train/test\n",
    "    \n",
    "    # from sklearn.preprocessing import StandardScaler\n",
    "    # scaler = StandardScaler()\n",
    "    # signal = scaler.fit_transform(outcome.shift(1) - outcome.shift(2))\n",
    "    # signal = round(signal)\n",
    "    \n",
    "    return signal\n",
    "\n",
    "def backtest_strategy_passive(outcome, initial_capital = 100000.0):\n",
    "    \"\"\"\n",
    "    Buy as much as possible at the beginning, sell everything at the end.\n",
    "    \"\"\"\n",
    "        \n",
    "    V0 = outcome.ix[ 0]\n",
    "    Vf = outcome.ix[-1]\n",
    "    \n",
    "    pct = (Vf - V0) / V0\n",
    "    # final = initial_capital * pct\n",
    "    \n",
    "    return pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the backtestings on the data, and see what is yielded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = df[symbol]\n",
    "# values = df[symbol].resample('1D').mean().pad()\n",
    "\n",
    "# Passive strategy\n",
    "return_final = backtest_strategy_passive(values)\n",
    "print(\"Return from passive strategy: {:.5f}\".format(return_final))\n",
    "\n",
    "# Strategy with recent history\n",
    "signals = strategy_with_recent_history(values)\n",
    "return_final = backtest_portfolio(values, signals)\n",
    "print(\"Return from strategy with recent history: {:.5f}\".format(return_final))\n",
    "\n",
    "# Random strategy\n",
    "signals = pd.Series(np.sign(np.random.randn(len(signals))), index = values.index)\n",
    "return_final = backtest_portfolio(values, signals)\n",
    "print(\"Return from random strategy: {:.5f}\".format(return_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further comparison, we implement a moving average crossover similar to [here](https://www.quantstart.com/articles/Backtesting-a-Moving-Average-Crossover-in-Python-with-pandas). We buy/sell when the short moving average crosses the long moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strategy_moving_average_crossover(bars, short_window = 100, long_window = 400):\n",
    "    \"\"\"    \n",
    "    Return the DataFrame of symbols containing the signals to go long, short or hold (1, -1 or 0).\n",
    "\n",
    "    Requires:\n",
    "    bars - A DataFrame of bars for the above symbol.\n",
    "    short_window - Lookback period for short moving average.\n",
    "    long_window - Lookback period for long moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    signals = pd.DataFrame(index = bars.index)\n",
    "    signals['invested'] = 0.\n",
    "\n",
    "    # Create the set of short and long simple moving averages over the respective periods\n",
    "    signals['short_mavg'] = pd.rolling_mean(bars, short_window, min_periods = 1)\n",
    "    signals['long_mavg'] = pd.rolling_mean(bars, long_window, min_periods = 1)\n",
    "\n",
    "    # Assign 1. when the short moving average is above the long moving average (else 0.)\n",
    "    signals['invested'] = np.where(signals['short_mavg'] > signals['long_mavg'], 1., 0.)\n",
    "    # Wait a little before investing\n",
    "    signals['invested'][:long_window] = 0.\n",
    "    \n",
    "    # Take the difference in order to generate buy/sell signal\n",
    "    signals['signal'] = signals['invested'].diff()\n",
    "\n",
    "    return signals['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a Moving Average Cross Strategy instance with a short moving\n",
    "# average window of 100 days and a long window of 400 days\n",
    "signals = strategy_moving_average_crossover(values, short_window = 100, long_window = 400)\n",
    "\n",
    "# Create a portfolio of AAPL, with $100,000 initial capital\n",
    "return_final = backtest_portfolio(values, signals)\n",
    "print(\"Return from moving average crossover strategy: {:.5f}\".format(return_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "We first need to select the features we want to use. We might want to use automated feature selection technique such as offered by [sklearn](http://scikit-learn.org/stable/modules/feature_selection.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Attach past/future values to given day and time\n",
    "\n",
    "# Given n_past values, predict the next n_future values.\n",
    "n_past = 5\n",
    "n_future = 1\n",
    "\n",
    "def columns_shift(df, col, r):\n",
    "    \"\"\"\n",
    "    Make shifted copies of series for each shift in in range.\n",
    "    \"\"\"\n",
    "    \n",
    "    dates = df.index\n",
    "    \n",
    "    # Upsample\n",
    "    # df_resampled = df[col].resample('1T').pad().to_frame(col)\n",
    "    df_resampled = df # to save memory\n",
    "    \n",
    "    # Create new columns\n",
    "    for i in r:\n",
    "        epoch = 'past' if i < 0 else 'future'\n",
    "        df_resampled[col + '_' + epoch + '_' + str(abs(i))] = df_resampled[col].shift(-i)\n",
    "    \n",
    "    # Pick only rows that were in the original data frame, and merge back\n",
    "    new_cols = df_resampled.ix[dates, :].drop(col, axis = 1)\n",
    "    df[new_cols.columns] = new_cols\n",
    "    # return pd.concat([df, new_cols], axis = 1)\n",
    "    return df\n",
    "\n",
    "# Drop prior columns\n",
    "# cols = [c for c in df.columns if c.lower()[:len('security_past_')] == 'security_past_']\n",
    "# df.drop(cols, axis = 1, inplace = True)\n",
    "\n",
    "# Create columns for past/future values (0 shift is not past)\n",
    "df = columns_shift(df, symbol, range(-n_past, n_future))\n",
    "\n",
    "# Create columns for past derivatives\n",
    "df = columns_shift(df, 'derivative', range(-n_past, -1))\n",
    "\n",
    "# Check out the table\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace = True) # Some models don't work with missing values\n",
    "\n",
    "# Select columns used as feature and target for machine learning\n",
    "target_columns = [col for col in df.columns if '_future_' in col]\n",
    "\n",
    "# feature = df[feature_columns]\n",
    "feature = df.drop(target_columns + [symbol, 'derivative'], axis = 1)\n",
    "target = df[target_columns]\n",
    "\n",
    "# The features and targets are:\n",
    "print(\"Features: {}\".format(list(feature.columns)))\n",
    "print(\"Targets: {}\".format(list(target.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hold out the last year as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_train_test(feature, target, cutoff_test = '2015-01-01'):\n",
    "    \"\"\"\n",
    "    Divide features and targets into train and test\n",
    "    \"\"\"\n",
    "\n",
    "    ind_test = df.index >= cutoff_test\n",
    "    feature_test = feature[ind_test]\n",
    "    target_test = target[ind_test]\n",
    "\n",
    "    ind_train = ~ind_test\n",
    "    feature_train = feature[ind_train]\n",
    "    target_train = target[ind_train]\n",
    "    \n",
    "    return feature_train, feature_test, target_train, target_test\n",
    "\n",
    "# Apply split\n",
    "feature_train, feature_test, target_train, target_test = split_train_test(feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try adding very many features, and then running an automated feature selection. Reducing the number of features might reduce high frequency oscillations in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# LassoCV since L1 norm promotes sparsity of features\n",
    "clf = LassoCV()\n",
    "sfm = SelectFromModel(clf, threshold = 1e-5)\n",
    "# sfm = SelectFromModel(clf, threshold = \"mean\")\n",
    "sfm.fit(feature_train, target_train)\n",
    "# NOTE had to disable mkl as discussed here: https://github.com/BVLC/caffe/issues/3884\n",
    "\n",
    "feature_kept = feature.columns[sfm.get_support()]\n",
    "print(\"Features: {}\".format(feature_kept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to bring this down to two features, and plot, as illustrated [here](http://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_boston.html#example-feature-selection-plot-select-from-model-boston-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = len(feature_kept)\n",
    "\n",
    "# Reset the threshold till the number of features equals two.\n",
    "# The attribute can be set directly instead of repeatedly fitting the metatransformer.\n",
    "X = feature_train\n",
    "while n_features > 2:\n",
    "    sfm.threshold += 10e-4\n",
    "    X_transform = sfm.transform(X)\n",
    "    n_features = X_transform.shape[1]\n",
    "\n",
    "feature_kept = feature.columns[sfm.get_support()]\n",
    "print(\"Features for threshold {}: {}\".format(sfm.threshold, feature_kept))\n",
    "\n",
    "try:\n",
    "    X_transform_df = pd.DataFrame(X_transform, columns = feature_kept)\n",
    "    sb.jointplot(*feature_kept, X_transform_df)  \n",
    "except IndexError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only most important features\n",
    "feature_train = pd.DataFrame(sfm.transform(feature_train), \n",
    "                             columns = feature_kept, index = feature_train.index)\n",
    "feature_test = pd.DataFrame(sfm.transform(feature_test), \n",
    "                            columns = feature_kept, index = feature_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first try a linear regression. We could have also tried a linear model with built-in feature selection like [Lasso or ElasticNet](http://scikit-learn.org/stable/modules/linear_model.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Quick cross validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(reg, feature, target, cv = 5)\n",
    "print(\"R^2 during CV: {:.2f} +/- {:.2f}\".format(scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our predictions, we introduce the Mean Absolute Percentage Error (MAPE), as often used in forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mape(outcome, predict):\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Percentage Error (MAPE) score. Positive, but lower is better.\n",
    "    \"\"\"\n",
    "    \n",
    "    outcome = np.array(outcome).ravel()\n",
    "    predict = np.array(predict).ravel()\n",
    "    \n",
    "    # Get only the NONZERO or NON-NAN elements\n",
    "    EPSILON = pow(10, -5)\n",
    "    idx = (np.abs(outcome) > EPSILON) | (~np.isnan(outcome)) | (~np.isnan(predict))\n",
    "    \n",
    "    # Extract those elements\n",
    "    outcome = outcome[np.where(idx)]\n",
    "    predict = predict[np.where(idx)]\n",
    "    \n",
    "    return np.mean(np.abs((outcome - predict) / outcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate, we also use the portfolio return obtained through the strategy using the predictions from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def portfolio_return_score(actual, predictions, get_portfolio = False):\n",
    "    \"\"\"\n",
    "    Compute returns from trading strategy with forecast.\n",
    "    \"\"\"\n",
    "    \n",
    "    # pred_test_pf = list(zip(*self.pred_test))[0]\n",
    "    # target_test_pf = self.target_test.ix[:,0]\n",
    "\n",
    "    signals = strategy_with_forecast(actual, predictions)\n",
    "    return backtest_portfolio(actual, signals, get_portfolio = get_portfolio)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce here a class for fitting an plotting a given model. We also output a few metrics [supported by sklearn](http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) along the mape score to evaluate the fit on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for fitting and plotting\n",
    "class Regression():\n",
    "    \n",
    "    def __init__(self, reg, feature_train, feature_test, target_train, target_test, **kwargs):\n",
    "        \n",
    "        self.reg = reg\n",
    "        self.feature_train = feature_train\n",
    "        self.feature_test = feature_test\n",
    "        self.target_train = target_train.ix[:,0]\n",
    "        self.target_test = target_test.ix[:,0]\n",
    "        \n",
    "        # Fit\n",
    "        self.fit(self.feature_train, self.target_train, **kwargs)\n",
    "        \n",
    "        # Predictions\n",
    "        self.pred_train = self.predict(feature_train)\n",
    "        self.pred_test = self.predict(feature_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        self.performance()\n",
    "    \n",
    "    def fit(self, feature, target, **kwargs):\n",
    "        \"\"\"\n",
    "        Train model.\n",
    "        \"\"\"\n",
    "        \n",
    "        start = clock()\n",
    "        self.reg.fit(feature, target, **kwargs)\n",
    "        print(\"Fit in {:.0f} seconds\".format(clock() - start))\n",
    "    \n",
    "    def predict(self, feature):\n",
    "        \"\"\"\n",
    "        Predict.\n",
    "        \"\"\"\n",
    "        \n",
    "        pred = self.reg.predict(feature)\n",
    "        \n",
    "        # FIXME ravel works for single output only\n",
    "        return pd.Series(pred.ravel(), index = feature.index)\n",
    "    \n",
    "    def performance(self):\n",
    "        \"\"\"\n",
    "        Output various scores to evaluate model.\n",
    "        \"\"\"\n",
    "        \n",
    "        target_test = self.target_test\n",
    "        pred_test = self.pred_test\n",
    "        \n",
    "        scores = {}\n",
    "    \n",
    "        scores['MAPE'] = self.mape = mape(target_test, pred_test)\n",
    "        \n",
    "        scores['Portfolio Return'], self.portfolio = \\\n",
    "            portfolio_return_score(target_test, pred_test, get_portfolio = True)\n",
    "    \n",
    "        from sklearn.metrics import r2_score\n",
    "        scores['R2'] = self.r2 = r2_score(target_test, pred_test)\n",
    "    \n",
    "        from sklearn.metrics import explained_variance_score\n",
    "        scores['Explained Variance'] = self.evs = explained_variance_score(target_test, pred_test)\n",
    "\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        scores['Mean Square Error'] = self.mse = mean_squared_error(target_test, pred_test)\n",
    "        scores['Root Mean Square Error'] = self.rmse = np.sqrt(self.mse)\n",
    "    \n",
    "        from sklearn.metrics import median_absolute_error\n",
    "        scores['Median Absolute Error'] = self.mae = median_absolute_error(target_test, pred_test)\n",
    "        \n",
    "        print(pd.Series(scores, name = 'Scores'))\n",
    "        \n",
    "    def plot_prediction(self, filename = None):\n",
    "        \"\"\"\n",
    "        Plot predicted versus actual.\n",
    "        \"\"\"\n",
    "        \n",
    "        def pack_pred(indices, target, pred):\n",
    "            \"\"\"\n",
    "            Pack in one data frame the target and the predictions.\n",
    "            \"\"\"\n",
    "        \n",
    "            df = pd.DataFrame(index = indices)\n",
    "            df['predicted'] = pred\n",
    "            df['security'] = target\n",
    "        \n",
    "            return df[['predicted', 'security']]\n",
    "        \n",
    "        # Plot only predictions on test set\n",
    "        df_test = pack_pred(self.feature_test.index, self.target_test, self.pred_test)\n",
    "        df_test.plot()\n",
    "        \n",
    "        # Add shaded region for error corresponding to training error\n",
    "        err_graph = self.rmse\n",
    "        plt.fill_between(df_test.index,\n",
    "                         df_test.predicted - err_graph, df_test.predicted + err_graph, alpha = 0.2)\n",
    "        \n",
    "        if filename is not None:\n",
    "            # Grab figure before show clear, then clear\n",
    "            plt.gcf().savefig(filename, bbox_inches = 'tight')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_residual(self):\n",
    "        \"\"\"\n",
    "        Plot residual\n",
    "        \"\"\"\n",
    "        \n",
    "        # residuals = np.square(self.target_test - self.pred_test)\n",
    "        residuals = (self.target_test - self.pred_test)**2  # NOTE may cause kernel crash\n",
    "        print(\"Max residual at {}\".format(residuals.idxmax()))\n",
    "        \n",
    "        residuals.plot(title = \"Residual\")\n",
    "        # plt.yscale('log')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_portfolio(self, filename = None):\n",
    "        \"\"\"\n",
    "        Plot portfolio.\n",
    "        \"\"\"\n",
    "        \n",
    "        # holdings_pos = self.portfolio.ix[self.portfolio['holdings'] > 0, 'holdings'].abs()\n",
    "        # holdings_neg = self.portfolio.ix[self.portfolio['holdings'] < 0, 'holdings'].abs()\n",
    "        # holdings_pos.plot(label = 'Positive Holdings')\n",
    "        # holdings_neg.plot(label = 'Negative Holdings')\n",
    "            \n",
    "        # self.portfolio['cash'].plot(title = 'Available Cash)\n",
    "        self.portfolio['total'].plot(title = 'Portfolio Total Value')\n",
    "        \n",
    "        if filename is not None:\n",
    "            # Grabs figure before show clears\n",
    "            plt.gcf().savefig(filename, bbox_inches = 'tight')\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression_linear = Regression(reg, feature_train, feature_test, target_train, target_test)\n",
    "regression_linear.plot_prediction(filename = 'figures/prediction.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression_linear.plot_residual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at which feature has a larger effect on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Peek at coefficients\n",
    "print(\"Coefficients: {}\".format(reg.coef_))\n",
    "print(\"Intercept: {}\".format(reg.intercept_))\n",
    "\n",
    "coef = pd.DataFrame(abs(reg.coef_), index = feature_train.columns)\n",
    "coef = coef.sort_values(by=[0], ascending=[False])\n",
    "\n",
    "coef.plot(kind = 'bar', legend = False, title = 'Absolute Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the forecast to test the trading strategy mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regression_linear.plot_portfolio(filename = 'figures/returns.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we look at the other strategies implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_test_pf = target_test.ix[:,0]\n",
    "\n",
    "# Passive strategy\n",
    "\n",
    "return_final = backtest_strategy_passive(target_test_pf)\n",
    "print(\"Return from passive strategy: {:.5f}\".format(return_final))\n",
    "\n",
    "# Strategy with recent history\n",
    "\n",
    "signals = strategy_with_recent_history(target_test_pf)\n",
    "return_final = backtest_portfolio(target_test_pf, signals)\n",
    "print(\"Return from strategy with recent history: {:.5f}\".format(return_final))\n",
    "\n",
    "# Create a Moving Average Cross Strategy\n",
    "\n",
    "signals = strategy_moving_average_crossover(target_test_pf, short_window = 100, long_window = 400)\n",
    "return_final = backtest_portfolio(target_test_pf, signals)\n",
    "print(\"Return from moving average crossover strategy: {:.5f}\".format(return_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_test_pf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with ElasticNet\n",
    "\n",
    "Let's see if we can improve the return of the portfolio by tweak parameters of our regression using cross-validation grid search from [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html). [Here](http://scikit-learn.org/stable/modules/grid_search.html) are tips on using grid search. We use elastic net, which is similar to a linear regression with an extra $L^1$ regularization term allowing for some built-in automatic feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "reg = ElasticNet()\n",
    "\n",
    "# List all parameters\n",
    "# print(reg.get_params().keys())\n",
    "\n",
    "parameters = {\n",
    "    'l1_ratio': [0.5, 1.],\n",
    "    'alpha': [0.5, 1.],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wrap the portfolio return score function for use with grid search\n",
    "from sklearn.metrics import make_scorer\n",
    "portfolio_return_scorer = make_scorer(portfolio_return_score, greater_is_better = True)\n",
    "\n",
    "# Grid search for the regression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "grid = GridSearchCV(reg, parameters, scoring = portfolio_return_scorer, verbose = 100)\n",
    "\n",
    "# Reset features to undo the automated feature selection\n",
    "feature_train, feature_test, target_train, target_test = split_train_test(feature, target)\n",
    "\n",
    "regression_linear = Regression(grid, feature_train, feature_test, target_train, target_test)\n",
    "regression_linear.plot_prediction()\n",
    "regression_linear.plot_residual()\n",
    "regression_linear.plot_portfolio()\n",
    "\n",
    "# What are the parameters with the best score?\n",
    "print(\"Best portfolio return: {:.5f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the portfolio return depends heavily on the set on which it is computed. Grid search with cross-validation uses three different test sets with each sets resulting in very different results on each sets -- but hopefully this may help make the model more robust.\n",
    "\n",
    "Let's look at the effect of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Peek at coefficients\n",
    "reg = grid.best_estimator_\n",
    "print(\"Coefficients: {}\".format(reg.coef_))\n",
    "print(\"Intercept: {}\".format(reg.intercept_))\n",
    "\n",
    "coef = pd.DataFrame(abs(reg.coef_), index = feature_train.columns)\n",
    "coef = coef.sort_values(by=[0], ascending=[False])\n",
    "\n",
    "coef.ix[coef.ix[:,0] > 1e-5, :].plot(kind = 'bar', legend = False, title = 'Absolute Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with XGBoost\n",
    "\n",
    "We use [XGBoost Python API](http://xgboost.readthedocs.io/en/latest/python/python_api.html) with cross-validation grid search from [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html). [Here](http://scikit-learn.org/stable/modules/grid_search.html) are tips on using grid search; and [here](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) for tweaking XGBoost.\n",
    "<img src=\"figures/gbdt.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "reg = XGBRegressor(silent = False, objective = 'reg:linear')\n",
    "\n",
    "# List all parameters\n",
    "# print(reg.get_params().keys())\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': [5, 10],\n",
    "    'learning_rate': [0.1],\n",
    "    # 'subsample': [0.1, 0.5],\n",
    "    'subsample': [0.2],\n",
    "    'colsample_bytree': [0.2], \n",
    "    # 'min_child_weight': [6], \n",
    "    'max_depth': [2, 4],\n",
    "    # 'scale_pos_weight': [10],\n",
    "    # 'gamma': [0], \n",
    "    # 'reg_alpha': [0], \n",
    "    # 'reg_lambda': [0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run grid search with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grid search for the regression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "grid = GridSearchCV(reg, parameters, scoring = portfolio_return_scorer, verbose = 100)\n",
    "\n",
    "# Reset features to undo the automated feature selection\n",
    "feature_train, feature_test, target_train, target_test = split_train_test(feature, target)\n",
    "\n",
    "regression_linear = Regression(grid, feature_train, feature_test, target_train, target_test)\n",
    "regression_linear.plot_prediction()\n",
    "regression_linear.plot_residual()\n",
    "regression_linear.plot_portfolio()\n",
    "\n",
    "# What are the parameters with the best score?\n",
    "print(\"Best portfolio return: {:.5f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the importance of each features according to XGBoost. [Here](https://www.kaggle.com/mmueller/liberty-mutual-group-property-inspection-prediction/xgb-feature-importance-python/code) is an example implementation of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = grid.best_estimator_\n",
    "importance = reg.booster().get_fscore()\n",
    "\n",
    "import operator\n",
    "importance = sorted(importance.items(), key = operator.itemgetter(1), reverse = True)\n",
    "\n",
    "importance = pd.DataFrame(importance, columns = ['feature', 'fscore'])\n",
    "importance['fscore'] = importance['fscore'] / importance['fscore'].sum()\n",
    "\n",
    "importance.plot(kind = 'bar', x = 'feature', y = 'fscore', legend = False)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "[TF Learn](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn) provide a sklearn-like interface to Google's Tensor Flow for neural networks. See [here](https://github.com/tflearn/tflearn/blob/master/tutorials/intro/quickstart.md) for a quick start. The particular neural network here has 3 layer network with 10, 20 and 10 hidden units respectively. The optimizer does not work well with features of different scales, so we have to scale to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural networks are sensitive to scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preproc = StandardScaler()\n",
    "feature_train.ix[:,:] = preproc.fit_transform(feature_train, target_train)\n",
    "feature_test.ix[:,:] = preproc.transform(feature_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In-depth logging\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Deep neural network with tensor flow\n",
    "import tensorflow.contrib.learn as learn\n",
    "\n",
    "# reg = learn.LinearRegressor() # simpler model, for debugging\n",
    "reg = learn.DNNRegressor(hidden_units = [10, 20, 10], model_dir = 'log/dnn2/')\n",
    "# For detailed log, run: ~/.local/bin/tensorboard --logdir=log/dnn1/\n",
    "\n",
    "# Parameters\n",
    "batch_size = None # NOTE Default is None and yields feature_train.shape[0]\n",
    "steps = 2000\n",
    "\n",
    "# Train and test\n",
    "regression_linear = Regression(reg, feature_train, feature_test, target_train, target_test, \n",
    "                               steps = steps, batch_size = batch_size)\n",
    "\n",
    "# Plot\n",
    "regression_linear.plot_prediction()\n",
    "regression_linear.plot_residual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute number of epochs (number of times the optimizer ran through all the training set)\n",
    "\n",
    "steps = reg.get_variable_value('global_step')\n",
    "batch_size = feature_train.shape[0] if batch_size is None else batch_size\n",
    "\n",
    "epoch = batch_size * steps / feature_train.shape[0]\n",
    "print(\"Epochs: {:.2f}\".format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the full interface of tensor flow would allow more fine tuning of many hyperparameters (but, if not for limited computational resources, the number of steps should also be increased). Here's the graph representation (obtained through tensorboard) of the neural network trained.\n",
    "<img src=\"figures/dnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We introduced various features like past values, past derivatives, rolling means for different windows, etc. Using automated feature selection (lasso in this case) allowed us to keep only the two most important features. We then applied a linear regression. Here is the values of the security and its predictions (with root mean square error band) tested on the last year of the data.\n",
    "\n",
    "<img src=\"figures/prediction.png\">\n",
    "\n",
    "We then used this forecast in a simple minute-by-minute trading strategy: (1) if the predicted value is higher than the previous actual value, we buy a fix number of shares; (2) if lower, than we sell the same fix number; (3) otherwise, we do nothing. Here is a plot of the total value of the portfolio in time using the strategy using the forecast.\n",
    "\n",
    "<img src=\"figures/returns.png\">\n",
    "\n",
    "In order to improve the portfolio return, we ran a grid search with cross-validation using the portfolio return as custom score to compare the various models. The grid search was applied to Elastic Net, but also to XGBoost. We also ran a simple neural network for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Notebook ran in {:.1f} minutes\".format((clock() - start_notebook)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
